<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Title: Application of Machine Learning to assessing correctness of performing weight lifting exercises</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<h2>Title: Application of Machine Learning to assessing correctness of performing weight lifting exercises</h2>

<p>Author: S. Begwani</p>

<h3>Overview</h3>

<p>The &ldquo;quantified self&rdquo; movement consists of people taking measurements of their physical parameters regularly to improve health, discover any repeating patterns, etc. Most studies focus on how much of a particular activity was performed, but do not consider how effectively it was performed. For the purpose of this assignment, we consider a data set from <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (Weight Lifting Exercise data set). </p>

<p>In this study, participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways under the supervised guidance of an experienced weight lifter. Data was collected from accelerometers on the belt, forearm, arm, and dumbbell of the 6 participants.</p>

<p>The training and the test data set have been made available for this assignment. Model building will be done on the training data set, and then the final model will be used to make predictions for the 20 test cases.</p>

<h3>Preliminary Analysis and Data Cleaning</h3>

<p>The following code assumes that the pml-training.csv, and the pml-testing.csv files are in the current working directory. The current working directory can be set using the setwd() command.</p>

<p>The training data is loaded. Clearly, a lot of the columns have missing values. To clean the data set, first we eliminate columns where 75% or more values are &#39;NA&#39; or missing or &ldquo;&rdquo;. It would not be wise to try imputation of values for these columns.</p>

<pre><code class="r">## Load the training data set
trainData &lt;- read.csv(&quot;pml-training.csv&quot;, header=TRUE,
                      na.strings=&quot;NA&quot;, stringsAsFactors=TRUE)
dim(trainData)
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<pre><code class="r">columnCollect &lt;- integer() ## Store the column numbers
## for those columns where 75% or more values are &quot;NA&quot; or NULL or &quot;&quot;
for (i in 1:ncol(trainData))
{
  noEmpty &lt;- sum(is.na(trainData[[i]]) | is.null(trainData[[i]]) 
                 | trainData[[i]]==&quot;&quot;)
  percEmpty &lt;- noEmpty/(nrow(trainData))
  if (percEmpty&gt;=0.75) columnCollect &lt;- c(columnCollect,i)  
}

trainData &lt;- trainData[,-columnCollect] ## Remove very sparse columns
dim(trainData) ## The output shows that nearly 100 columns have been removed
</code></pre>

<pre><code>## [1] 19622    60
</code></pre>

<pre><code class="r">## We now perform a quick check to see how many NA values remain in the data
## The results show there are no such values
any(is.na(trainData))
</code></pre>

<pre><code>## [1] FALSE
</code></pre>

<p>Next, the data set is examined to see if there are any variables with near zero variance. Also, it is important to examine if there are factor variables which have a huge dominance of one class value over others. This can be seen through the frequency ratio of the most commonly occuring class value to the next commonly occurring class value. While there are no clear guidelines, if the ratio is above 50, it indicates that the distribution of levels within that factor is very highly skewed, and this can create problems in analysis. In particular, algorithms will find it very difficult to maintain equal class distribution (distribution of different levels within a factor) between the training and validation set.</p>

<pre><code class="r">library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code class="r">zeroVar &lt;- nearZeroVar(trainData, saveMetrics=TRUE)
k &lt;- zeroVar[zeroVar$freqRatio&gt;=50 | zeroVar$nzv==TRUE,]
k
</code></pre>

<pre><code>##               freqRatio percentUnique zeroVar   nzv
## new_window        47.33       0.01019   FALSE  TRUE
## roll_arm          52.34      13.52563   FALSE FALSE
## pitch_arm         87.26      15.73234   FALSE FALSE
## pitch_forearm     65.98      14.85577   FALSE FALSE
</code></pre>

<pre><code class="r">trainData &lt;- trainData[,- which(names(trainData) %in% rownames(k))] 
</code></pre>

<p>As can be seen, four variables qualify. These variables are removed from the training set.</p>

<p>Finally, by manual inspection, we can remove some variables as they are unlikely to have any bearing on the value of the &ldquo;classe&rdquo; variable. These variables are:</p>

<p>First column - serial number of rows
user_name - The specific user does not matter as the volunteers performed the exercises and simulated the mistakes under the supervision of an experienced weight lifter as mentioned in the write-up at <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>. 
raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp - There is no reason to believe that the timing of the exercise had anything to do with the results</p>

<p>We eliminate these columns via the following code:</p>

<pre><code class="r">trainData &lt;- trainData[,-(1:5)] ## As the abovementioned columns are contiguous
</code></pre>

<h3>Principal Components Analysis</h3>

<p>The trainData data set now has 19622 cases with each case having 51 variables.</p>

<p>Before undertaking PCA, we examine how correlated the different variables are. In particular, we examine which variables have a correlation of more than 0.8.</p>

<pre><code class="r">M &lt;- abs(cor(trainData[,-51])) ## creates a matrix where the diagonal acts as a mirror for the two sides
diag(M) &lt;- 0 ## Setting the diagonal terms to zero
nrow (which(M&gt;0.8,arr.ind=TRUE)) / 2 ## Number of distinct variable pairs that are correlated
</code></pre>

<pre><code>## [1] 19
</code></pre>

<p>As can be seen from the output, 19 pairs of variables are highly correlated. A PCA approach can produce a huge reduction in the number of variables while still capturing most of the variation in the data.</p>

<p>In order to perform PCA, it is important that the data must be near-Gaussian. The predictor variables will be centered and scaled to achieve this. For this, it must be checked whether all the predictor variables are integer or numeric. Note: We cannot use logarithmic, or Box-Cox transformations as several variables have negative values. </p>

<pre><code class="r">all(sapply(trainData[,-51],class)==&quot;integer&quot; 
    | sapply(trainData[,-51],class)==&quot;numeric&quot;)
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<p>Since we have all predictor variables of type numeric or integer, we can now apply principal component analysis.</p>

<p>We shall use the preProcess() function, using method=&ldquo;pca&rdquo;, and aim for retention of 90% variation in data by setting the thresh parameter to 0.90.</p>

<p>Retaining only 90% of the variation potentially means that we are throwing away some valuable information. The threshold can be set to higher values, say 0.95, or even 0.99 as it would probably be in real-life applications. However, the number of principal components, and hence the data size, increases, and this causes memory issues on my machine when running a bagged classification tree (this is explained later).</p>

<p>As such, with only 90% of the variance in data retained, it is logical to assume that the out-of-sample error rate will be greater than 10% or the accuracy will be less than 90% on out-of-sample data - more accurate predictions of out-of-sample errors will be ascertained during the model fit process.</p>

<pre><code class="r">train1Data &lt;- trainData ## Create a copy of trainData

preProc &lt;- preProcess(train1Data[,-51],method=c(&quot;center&quot;,&quot;scale&quot;,&quot;pca&quot;), 
                      thresh=0.90)
train1Data &lt;- predict(preProc,train1Data[,-51])
ncol(train1Data)
</code></pre>

<pre><code>## [1] 18
</code></pre>

<p>The number of principal components required to retain a variance of 90% is 18 as shown by the code above.</p>

<h3>Model Fitting, Cross-Validation and Final Model</h3>

<p>A bagged classification tree is fitted to the training data. For this, the training data is partitioned 60:40 between the training set and the validation set. Since the test set is separate, the model would not be biased by training the model on the training set and subequently validating against the cross-validation set to find the best-fit model.</p>

<p>In this case, we change the value of the number of bags, B, to ascertain the best model. In the bagging approach, the number of trees B is a free parameter that can be optimized.</p>

<p>In this case, the model is fitted for values of B between 5 and 9. Due to memory constraints, values above 9 cannot be evaluated, as a 9-bag model consumes about 3 GB of physical memory.</p>

<pre><code class="r">set.seed(333)
inTrain &lt;- createDataPartition(y=trainData$classe,p=0.6,list=FALSE)
trainingSet &lt;- train1Data[inTrain,]
validationSet &lt;- train1Data[-inTrain,]

## The model is fitted for bag values, B, between 5 to 9.
modelTable &lt;- data.frame()

for (i in 5:9)
{
  ## Fit the model
  treebag &lt;- bag(trainingSet, trainData[inTrain,]$classe, B=i,
                 bagControl = bagControl(fit = ctreeBag$fit, 
                                         predict=ctreeBag$pred, 
                                         aggregate=ctreeBag$aggregate))
  ## Calculate the confusion matrix

  predictedValues &lt;- predict(treebag,validationSet)
  trueValues &lt;- trainData[-inTrain,]$classe
  confMat &lt;- confusionMatrix(predictedValues,trueValues)

  ## Store in the data frame

  modelTable &lt;- rbind(modelTable, c(i, confMat$overall[&quot;Accuracy&quot;]))

  ## Remove the existing bagged model to free up space for the next run

  rm(treebag)

}

colnames(modelTable) &lt;- c(&quot;Number of Bags&quot;, &quot;Accuracy&quot;)
modelTable
</code></pre>

<pre><code>##   Number of Bags Accuracy
## 1              5   0.8339
## 2              6   0.8459
## 3              7   0.8581
## 4              8   0.8608
## 5              9   0.8674
</code></pre>

<p>As can be seen from the above table, the accuracy increases with the number of bags employed. A value of B=9 is chosen to construct the final model.</p>

<p>With a value of B=9, the accuracy of the model is about 86%. This is also what the estimated out-of-sample accuracy rate for the final model is likely to be. In other words, the expectation is that the final model will have an error rate of around 15%, or in the case of this specific assignment, this model should classify approximately 17 of the 20 cases correctly in the testing data set.</p>

<p>** IMPORTANT: While fitting the final model, the entire training data set must be utilized. However, that would lead to an increase in data size (100% as opposed to 60% of the training set), and the memory requirements would increase sizeably. In this analysis, the final model with B=9 is derived from 60% of the training set as was done in the code above.</p>

<pre><code class="r">finalModel &lt;- bag(trainingSet, trainData[inTrain,]$classe, B=9,
                 bagControl = bagControl(fit = ctreeBag$fit, 
                                         predict=ctreeBag$pred, 
                                         aggregate=ctreeBag$aggregate))
</code></pre>

<h3>Predicting values for the testing set</h3>

<p>Before predicting values, we will have to first apply the same data cleaning and preprocessing steps to the testing data set. Then, we make the predictions for the 20 test cases.</p>

<pre><code class="r">testData &lt;- read.csv(&quot;pml-testing.csv&quot;, header=TRUE,
                      na.strings=&quot;NA&quot;, stringsAsFactors=TRUE)
dim(testData)
</code></pre>

<pre><code>## [1]  20 160
</code></pre>

<pre><code class="r">## Remove the columns which were removed in the training set

testData &lt;- testData[,-columnCollect]
testData &lt;- testData[,- which(names(testData) %in% rownames(k))]
testData &lt;- testData[,-(1:5)]

## Quick check to see dimensions of the test set and presence of NA values

dim(testData)
</code></pre>

<pre><code>## [1] 20 51
</code></pre>

<pre><code class="r">any(is.na(testData))
</code></pre>

<pre><code>## [1] FALSE
</code></pre>

<pre><code class="r">## The test data set has no missing values, but the number of columns is 
## 51 despite not having the classe variable.
## A quick look at the test data set reveals that the last column is
## problem_id, which can be removed

testData &lt;- testData[,-ncol(testData)]
dim(testData)
</code></pre>

<pre><code>## [1] 20 50
</code></pre>

<pre><code class="r">## Apply preprocessing step using the preProc object created 
## with the training set

test1Data &lt;- predict(preProc,testData)

## We can now make predictions using our final fitted model

predict(finalModel,test1Data)
</code></pre>

<pre><code>##  [1] B A A A A C D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

</body>

</html>

